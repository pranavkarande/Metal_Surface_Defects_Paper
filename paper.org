#+TITLE: *Classification of Metal Surface Defects using Convolutional Neural Network*
#+AUTHOR:
#+LaTeX_class: article
#+LATEX_CLASS_OPTIONS: [a4paper, 11pt, twocolumn]
#+LATEX_HEADER: \usepackage[margin=0.7in]{geometry}
#+OPTIONS: toc:nil

#+LATEX_HEADER: \include{~/code/ML/Document/authors.tex}

#+BEGIN_abstract
The implementation of defect detection depends on a special detection dataset. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification-ability, this system employs a baseline convolution neural network (CNN). We explore optimized values of parameters using the Sequential Convolutional Neural Network model to classify the defects correctly without causing over-fitting.
#+END_abstract

* Introduction
Steel is one of the most important building materials of modern times and the production process of flat sheet steel is complicated. Before shipping or delivering steel, sheets need to undergo a careful inspection procedure to avoid defects and thus localizing and classifying surface defects on a steel sheet is crucial.  Typically, in steel mills, human inspectors manually perform the defect detection process on steel sheets. However, this procedure is very time consuming, costly but
lower efficient and does not meet up the requirement of realtime online defect detection. Further, with the rise of Automation, IoT and Robotics, the need for software to be able to make trivial jobs like defect detetion and classification autonomous has increased. The defect detection system in steel sheet surfaces is playing a critical role in the steel sheets industry by detecting, localizing, recognizing and subsequently correcting causative factors. It is also quite necessary for controlling product quality and generating real-time analysis reports. Detecting process involves determining the existence of the steel surface defects from images taken from the industrial cameras.
In this study, we advance the steel defect inspection methods by designing machine learning models that aim to detect multi-level defects from sample steel sheet images and classify them according to their corresponding classes. This paper presents a simplified approach to serve the above purpose using the basic Machine Learning (ML) packages such as TensorFlow and Keras.

* Dataset
NEU surface defects dataset is a defect classification dataset that consists of six types of defects from hot-rolled steel plates, including crazing, inclusion, patches, pitted surface, rolled-in scales and scratches. Each class has 300 images (1800 images in total), but it does not mean that an image consists of a single defect. Each image is a 224x224 grayscale image . In any image, a steel sheet with some kind of defect can be seen. This data is splitted for training which is dscribed in the Training section. Examples of defect images are shown in Fig. 5.

* Incorporated Packages
** Tensorflow
TensorFlow, an interface for expressing machine learning algorithms, is utilized for implementing ML systems into fab- rication over a bunch of areas of computer science, including sentiment analysis, voice recognition, geographic information extraction, computer vision, text summarization, information retrieval, computational drug discovery and flaw detection to pursue research [18]. In the proposed model, the whole Sequential CNN architecture (consists of several layers) uses TensorFlow at backend. It is also used to reshape the data (image) in the data processing.
** Keras
Keras gives fundamental reflections and building units for creation and transportation of ML arrangements with high iteration velocity. It takes full advantage of the scalability and cross-platform capabilities of TensorFlow. The core data structures of Keras are layers and models [19]. All the layers used in the CNN model are implemented using Keras. Along with the conversion of the class vector to the binary class matrix in data processing, it helps to compile the overall model.

** Pyplot
Pyplot is a part of Matplotlib Python library. It was used to print the image in the script and check its dimensions to feed them into the neural network.

** VisualKeras
Visualkeras library was used to plot the arcjitecture of the designed CNN model .

** OS
The OS library was used to traverse through the image directory for training, EDA as well as testing purposes.

* Training
Training the CNN model required _ Steps:
** Setting Up the Data
Although the data was already in the splitted format, the contents of the data were again altered. Three directories - Training, Testing and Validation were created and in the training and validation directories 6 directories, each representing one class label were added in which the respective images were stored. /The data was split in three ways during training : [80 - 20, 75- 25 and 85-15]./
However, a slight change was made in the testing directory, which initially had images split based on classes. This directory was converted such that it only has a randm set of images that are not seperated by class labels.
** Image Augmentation
Once the data from the directory is read, ImageDataGenerator is used to augment the images as per requirement to icrease the scale of training and prepare better scenarios for the model to train. The images were rescaled by a factor of 1/255 . The augmentation of images was done in form of horizontal and vertical flips, rotations in the range of 0 degree to 30 degrees, horizontal shear , width shift as well as height shift. 
#Code Snippet of Data Generator
** Creating The Model
Now, to start training, the CNN model was created using Sequential() function in Tensorflow and the model Architecture was built as follows:
 - 
The Model can be visualized as follows using visualkeras and PIL:[[./WhatsApp Image 2022-07-22 at 10.07.41 AM.jpeg]]
#Architecture SS
** Setting the Epochs and Steps per Epoch
This step required the most trial and error as every time the model is run, there is slight difference in the validation accuracy achieved. To beat the current Threshhold of 98.6% accuracy,the training using model.fit_generator() function was done multiple times changing the epochs, steps per epoch and validation steps. To come out of this loophole of Training multiple times, Keras Tuner was used for Model Optimization
** Setting Up the GPU 
To make the training process faster and more efficient, there was a need to set up the GPU in the laptop. 

* Testing and Result
A seperate set of images that is not a part of the training data was kept for testing to make sure that the model is not an overfit on the training data. 
The Convolutional Neural Network designed for the training of this dataset consisted of 3 convolution and maxpool layers, 2 dense (fully connected )layers and 2   
dropout layers with a dropout rate of 0.2. The model was trained using Model.fit_generator() function as image augmentation was done using ImageDataGenerator library.
With the Nvidia GTX 1650-Ti GPU, the model took 15 miutes to run completely and give an accuracy of 99.3%.
Once the training is complete, the OS library was used to move to the directory with the testing images. Model.predict() function was used get a class labels vector as an output which had 0s except for one index which was 1, indicating presence of that defect. The testing dataset consisted of 72 images (12 for each class) and while testing, 68/72 predictions were correctly made by the model. Thus, since the vaidation accuracy was 99.3% and the testing accuracy is also around 95%, it can be said that the model is not an overfit on the training data. 
The model consisting of only 13 layers was able to provide a 99.3% accuracy without causing any overfitting. Thus, this model can be a very good model for Low-End systems that cannot run huge models like Res-Net 50+ or VGG19 for their data. 
[[./WhatsApp Image 2022-08-11 at 8.17.36 AM.jpeg]]

* Experiments and Analysis
In search of a good valisation accuracy, we engaged in multiple experiments on out CNN model and Dataset . 
- We tried using Keras_Tuner library to find the best Hyperparameters for our CNN model. Keras Tuner runs a python script where, provided a certain range of hyperparameter values, it checks which of them give the best valiation accuracy on the data by running Random Search as well as Grid Search algorithms. However, Grid Search algorithm requires a really strong GPU and system to succesfull run and give a result as it does exhaustive search on the gicen range. Further, Random search model as it sounds, uses random values from the given ranges due to which, it would take a lot of time to give us good hyperparameter values which is again not necessary to be the best value. Another problem with Random Search is that if in case any values lead to Overfitting of the Model (Validation Accuracy 100%), then any other tests done after that will not be able to give us a result since no other values will be recorded as better values.
- We tried Cross Validation of our model to see if it would give us a better average value. We used K-fold cross validation , created 5 folds (20% testing data). The 5 training instances gave out accuracies of 98.2%, 94,3%, 97.4%, 82.9% and 99.1% making the average accuracy as 94.8% which is far away from what we got training our model. Further, this method again was extremely time consuming since each instance took time comparable to how much it took for our model to run once. And still we were unable to get a better validation accuracy value.
- We tried using heavier models on the data like VGG 16, VGG 19 as well as Res-Net 50+ to see if they would give us better results.However, it was seen that the accuracies generated were extremely low, there was a clear underfitting of models on the dataset we had. Further, the system took a really loog time on each epoch even with GPU and crashes the system multiple times proving how difficult it is to run such heavy models on normal systems, something that our model is able to overcome

* Conclusion

* References
